{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this lab we will experiment with recurrent neural networks. These are a useful type of model for predicting sequences or handling sequences of things as inputs. We will implement them in Keras+Tensorflow but many implementations can be found online with many sets of variants. Here are installation instructions for Keras: https://keras.io/#installation, and here are installation instructions for Tensorflow: https://github.com/tensorflow/tensorflow#download-and-setup. You should also be able to run those from a Docker container.\n",
    "\n",
    "We will take a set of 10 thousand image descriptions from the MS-COCO dataset (400,000 sentences) and make our recurrent network learn how to compose new sentences character by character.\n",
    "You can download this data here: http://www.cs.virginia.edu/~vicente/recognition/captions_train.txt.zip\n",
    "\n",
    "First, let's import libraries and make sure you have everything properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the Text\n",
    "We will first read the sentences and map each character to a unique identifier so that we can treat each sentence as an array of character ids. The code below loads the captions from a text file and places them inside a caption tensor that is a matrix of size numCaptions x maxCaptionLength x charVocabularySize. We will also create a caption tensor that contains the sentences but shifted by one character. Each character is mapped to an incremental ID, so we keep two hashmaps to convert from character to id and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30724\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_recipes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3b33e931397c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar2id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'char2id_2.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2char\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'id2char_2.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_recipes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cleaned_recipes_2.p'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_recipes' is not defined"
     ]
    }
   ],
   "source": [
    "all_recipes = pickle.load(open('cleaned_recipes.p', 'rb'))\n",
    "def booze_permuter(recipes):\n",
    "    for rec in recipes:\n",
    "        ings = rec['ingredients']\n",
    "        i = 0\n",
    "        while i < len(ings) and is_number(ings[i][0]):\n",
    "            i += 1\n",
    "            \n",
    "        ing_list = ings[:i]\n",
    "        garn_list = ings[i:]\n",
    "        \n",
    "        random.shuffle(ing_list)\n",
    "        random.shuffle(garn_list)\n",
    "        \n",
    "        ing_list.extend(garn_list)\n",
    "        yield '\\n'.join(ing_list)\n",
    "        \n",
    "def get_new_recipes():\n",
    "    recipe_list = []\n",
    "    for recipe in booze_permuter(all_recipes):\n",
    "        recipe_list.append(recipe)\n",
    "    return recipe_list\n",
    "\n",
    "def training_set_generator(num_sets):\n",
    "    for i in range(num_sets):\n",
    "        pass\n",
    "        \n",
    "test_set = get_new_recipes()\n",
    "print(len(test_set))\n",
    "# Compute a char2id and id2char vocabulary.\n",
    "char2id = {}\n",
    "id2char = {}\n",
    "charIndex = 0\n",
    "for recipe in test_set:\n",
    "    for char in recipe:\n",
    "        if char not in char2id:\n",
    "            char2id[char] = charIndex\n",
    "            id2char[charIndex] = char\n",
    "            charIndex += 1\n",
    "\n",
    "# Add a special starting and ending character to the dictionary.\n",
    "char2id['S'] = charIndex; id2char[charIndex] = 'S'  # Special sentence start character.\n",
    "char2id['E'] = charIndex + 1; id2char[charIndex + 1] = 'E'  # Special sentence ending character.\n",
    "pickle.dump(char2id, open('char2id_2.p', 'wb'), protocol = 2)\n",
    "pickle.dump(id2char, open('id2char_2.p', 'wb'), protocol = 2)\n",
    "pickle.dump(all_recipes, open('cleaned_recipes_2.p', 'wb'), protocol = 2)\n",
    "\n",
    "\n",
    "# max_recipe_length = 500\n",
    "# # Place test_set inside tensors.\n",
    "# maxSequenceLength = max_recipe_length + 1\n",
    "# # inputChars has one-hot encodings for every character, for every caption.\n",
    "# inputChars = np.zeros((len(test_set), maxSequenceLength, len(char2id)), dtype=np.bool)\n",
    "# # nextChars has one-hot encodings for every character for every caption (shifted by one).\n",
    "# nextChars = np.zeros((len(test_set), maxSequenceLength, len(char2id)), dtype=np.bool)\n",
    "\n",
    "# for i in range(0, len(test_set)):\n",
    "#     inputChars[i, 0, char2id['S']] = 1\n",
    "#     nextChars[i, 0, char2id[test_set[i][0]]] = 1\n",
    "#     for j in range(1, maxSequenceLength):\n",
    "#         if j < len(test_set[i]) + 1:\n",
    "#             inputChars[i, j, char2id[test_set[i][j - 1]]] = 1\n",
    "#             if j < len(test_set[i]):\n",
    "#                 nextChars[i, j, char2id[test_set[i][j]]] = 1\n",
    "#             else:\n",
    "#                 nextChars[i, j, char2id['E']] = 1\n",
    "#         else:\n",
    "#             inputChars[i, j, char2id['E']] = 1\n",
    "#             nextChars[i, j, char2id['E']] = 1\n",
    "\n",
    "# print(\"input:\")\n",
    "# print(inputChars.shape)  # Print the size of the inputCharacters tensor.\n",
    "# print(\"output:\")\n",
    "# print(nextChars.shape)  # Print the size of the nextCharacters tensor.\n",
    "# print(\"char2id:\")\n",
    "# print(char2id)  # Print the character to ids mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Note:</b> In order to clearly show how inputChars, and nextChars store the sequences, let's try printing a sentence back from its stored format in these two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'S' u'0' u'.' u'2' u'5' u' ' u'p' u'a' u'r' u't' u' ' u'g' u'i' u'n'\n",
      " u'\\n' u'0' u'.' u'0' u'6' u'2' u'5' u' ' u'p' u'a' u'r' u't' u's' u' '\n",
      " u'f' u'r' u'e' u's' u'h' u' ' u'l' u'i' u'm' u'e' u' ' u'j' u'u' u'i' u'c'\n",
      " u'e' u'\\n' u'1' u' ' u'p' u'a' u'r' u't' u's' u' ' u'm' u'i' u's' u't'\n",
      " u' ' u't' u'w' u's' u't' u' ' u'c' u'r' u'a' u'n' u'b' u'e' u'r' u'r' u'y'\n",
      " u'\\xae' u'\\n' u'0' u'.' u'2' u'5' u' ' u'p' u'a' u'r' u't' u' ' u'v' u'o'\n",
      " u'd' u'k' u'a' u'\\n' u'c' u'r' u'a' u'n' u'b' u'e' u'r' u'r' u'i' u'e'\n",
      " u's' u' ' u'a' u'n' u'd' u' ' u'l' u'e' u'm' u'o' u'n' u' ' u'o' u'r' u' '\n",
      " u'l' u'i' u'm' u'e' u' ' u's' u'l' u'i' u'c' u'e' u's' u' ' u'f' u'o' u'r'\n",
      " u' ' u'g' u'a' u'r' u'n' u'i' u's' u'h' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E']\n",
      "[u'0' u'.' u'2' u'5' u' ' u'p' u'a' u'r' u't' u' ' u'g' u'i' u'n' u'\\n'\n",
      " u'0' u'.' u'0' u'6' u'2' u'5' u' ' u'p' u'a' u'r' u't' u's' u' ' u'f' u'r'\n",
      " u'e' u's' u'h' u' ' u'l' u'i' u'm' u'e' u' ' u'j' u'u' u'i' u'c' u'e'\n",
      " u'\\n' u'1' u' ' u'p' u'a' u'r' u't' u's' u' ' u'm' u'i' u's' u't' u' '\n",
      " u't' u'w' u's' u't' u' ' u'c' u'r' u'a' u'n' u'b' u'e' u'r' u'r' u'y'\n",
      " u'\\xae' u'\\n' u'0' u'.' u'2' u'5' u' ' u'p' u'a' u'r' u't' u' ' u'v' u'o'\n",
      " u'd' u'k' u'a' u'\\n' u'c' u'r' u'a' u'n' u'b' u'e' u'r' u'r' u'i' u'e'\n",
      " u's' u' ' u'a' u'n' u'd' u' ' u'l' u'e' u'm' u'o' u'n' u' ' u'o' u'r' u' '\n",
      " u'l' u'i' u'm' u'e' u' ' u's' u'l' u'i' u'c' u'e' u's' u' ' u'f' u'o' u'r'\n",
      " u' ' u'g' u'a' u'r' u'n' u'i' u's' u'h' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E'\n",
      " u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E' u'E']\n"
     ]
    }
   ],
   "source": [
    "trainCaption = inputChars[25, :, :]  # Pick some caption\n",
    "labelCaption = nextChars[25, :, :]  # Pick what we are trying to predict.\n",
    "\n",
    "def printCaption(sampleCaption):\n",
    "    charIds = np.zeros(sampleCaption.shape[0])\n",
    "    for (idx, elem) in enumerate(sampleCaption):\n",
    "        charIds[idx] = np.nonzero(elem)[0].squeeze()\n",
    "    print(np.array([id2char[x] for x in charIds]))\n",
    "\n",
    "printCaption(trainCaption)\n",
    "printCaption(labelCaption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, you will notice that the sentences are indeed shifted. This is because we are going to predict the next character at each timestep. The first character is 'S' which means start of sentences, and the next character in our target should be 'a' which is the first actual character of the sentence. The later characters in the sentence will also use the \"history\" of all previous characters to find out what goes next. \n",
    "\n",
    "## 2. Building our model using an LSTM Recurrent Network.\n",
    "Next we will create a recurrent neural network using Keras which takes an input set of characters (one-hot encoded) of size (batch_size, maxSequenceLength, charVocabularySize), similarly the output of this network will be a vector of size (batch_size, maxSequenceLength, charVocabularySize). However, the output does not contain one-hot encodings. The output contains a probability distribution (the output of a softmax) for every time step in the sequence. We see in section 4 how to decode the sequence from this distribution, you can just take the character corresponding to the index with the max probability for every time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 501, 128)      131584      lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 501, 128)      16512       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(None, 501, 128)      0           timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(None, 501, 128)      16512       timedistributed_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_4 (TimeDistribute(None, 501, 128)      0           timedistributed_3[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 164608\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Sample input Batch size: (32, 501, 128)\n",
      "Sample input Batch labels (nextChars): (32, 501, 128)\n",
      "Output Sequence size: (32, 501, 128)\n"
     ]
    }
   ],
   "source": [
    "print('Building training model...')\n",
    "hiddenStateSize = 128\n",
    "hiddenLayerSize = 128\n",
    "model = Sequential()\n",
    "# The output of the LSTM layer are the hidden states of the LSTM for every time step. \n",
    "model.add(LSTM(hiddenStateSize, return_sequences = True, input_shape=(maxSequenceLength, len(char2id))))\n",
    "# Two things to notice here:\n",
    "# 1. The Dense Layer is equivalent to nn.Linear(hiddenStateSize, hiddenLayerSize) in Torch.\n",
    "#    In Keras, we often do not need to specify the input size of the layer because it gets inferred for us.\n",
    "# 2. TimeDistributed applies the linear transformation from the Dense layer to every time step\n",
    "#    of the output of the sequence produced by the LSTM.\n",
    "model.add(TimeDistributed(Dense(hiddenLayerSize)))\n",
    "model.add(TimeDistributed(Activation('relu'))) \n",
    "model.add(TimeDistributed(Dense(len(char2id))))  # Add another dense layer with the desired output size.\n",
    "model.add(TimeDistributed(Activation('softmax')))\n",
    "# We also specify here the optimization we will use, in this case we use RMSprop with learning rate 0.001.\n",
    "# RMSprop is commonly used for RNNs instead of regular SGD.\n",
    "# See this blog for info on RMSprop (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "# categorical_crossentropy is the same loss used for classification problems using softmax. (nn.ClassNLLCriterion)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(lr=0.001))\n",
    "\n",
    "print(model.summary()) # Convenient function to see details about the network model.\n",
    "\n",
    "# Test a simple prediction on a batch for this model.\n",
    "print(\"Sample input Batch size:\"),\n",
    "print(inputChars[0:32, :, :].shape)\n",
    "print(\"Sample input Batch labels (nextChars):\"),\n",
    "print(nextChars[0:32, :, :].shape)\n",
    "outputs = model.predict(inputChars[0:32, :, :])\n",
    "print(\"Output Sequence size:\"),\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model\n",
    "Keras already implements a generic trainModel functionality through the model.fit function, but it also contains model.train_on_batch if you want to perform the training for loop yourself. For more informations about Keras model functionalities you can see here: https://keras.io/models/model/\n",
    "\n",
    "If you installed Tensorflow with GPU support, this will automatically run on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30724/30724 [==============================] - 650s - loss: 1.1430   \n",
      "Epoch 2/10\n",
      "30724/30724 [==============================] - 650s - loss: 0.6997   \n",
      "Epoch 3/10\n",
      "30724/30724 [==============================] - 637s - loss: 0.5498   \n",
      "Epoch 4/10\n",
      "30724/30724 [==============================] - 629s - loss: 0.4638   \n",
      "Epoch 5/10\n",
      "30724/30724 [==============================] - 629s - loss: 0.4070   \n",
      "Epoch 6/10\n",
      "30724/30724 [==============================] - 674s - loss: 0.3692   \n",
      "Epoch 7/10\n",
      "30724/30724 [==============================] - 803s - loss: 0.3432   \n",
      "Epoch 8/10\n",
      "30724/30724 [==============================] - 832s - loss: 0.3249   \n",
      "Epoch 9/10\n",
      "30724/30724 [==============================] - 830s - loss: 0.3115   \n",
      "Epoch 10/10\n",
      "30724/30724 [==============================] - 802s - loss: 0.3011   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0c9272a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputChars, nextChars, batch_size = 128, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verifying the Model is indeed Learning\n",
    "Here we input an arbitrary caption from the training set (one-hot encoded), compute the output using the trained model, and decode this output back into a char array. Ideally we should see the same input caption shifted by one character. However you would need to run the training code for around 24 hours straight to get the model close to that point (it is ok if you only run the model for 10 iterations for the purposes of this lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('cocktail_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55 clu oz. mhter\n",
      "0 55 clu oz. mocntreau lr coiple sec\n",
      "0.55 clu sz. moandy\n",
      "0.5 tlu oz. mreshllimon juice\n",
      "0 55 clu oz. mpple cider\n",
      "(occentrate\n",
      "EEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEEE\n"
     ]
    }
   ],
   "source": [
    "# Test a simple prediction on a batch for this model.\n",
    "captionId = 132\n",
    "\n",
    "inputCaption = inputChars[captionId:captionId+1, :, :]\n",
    "outputs = model.predict(inputCaption)\n",
    "\n",
    "# printCaption(inputCaption[0])\n",
    "print(''.join([id2char[x.argmax()] for x in outputs[0, :, :]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Inference Model.\n",
    "We verified in the previous section that the model was somewhat working on training data. However, we want to be able to create new sentences from this model starting from zero. We want to use the same parameters of the trained model to produce text character by character. We build here such model and just copy the parameters from our trained model above. We show in the following section (section 6) how to produce the sentences using this inference_model. Please pay attention to all the comments in the code below to see what are the differences with the model at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Inference model...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The only difference with the \"training model\" is that here the input sequence has \n",
    "# a length of one because we will predict character by character.\n",
    "print('Building Inference model...')\n",
    "inference_model = Sequential()\n",
    "# Two differences here.\n",
    "# 1. The inference model only takes one sample in the batch, and it always has sequence length 1.\n",
    "# 2. The inference model is stateful, meaning it inputs the output hidden state (\"its history state\")\n",
    "#    to the next batch input.\n",
    "inference_model.add(LSTM(hiddenStateSize, batch_input_shape=(1, 1, len(char2id)), stateful = True))\n",
    "# Since the above LSTM does not output sequences, we don't need TimeDistributed anymore.\n",
    "inference_model.add(Dense(hiddenLayerSize))\n",
    "inference_model.add(Activation('relu'))\n",
    "inference_model.add(Dense(len(char2id)))\n",
    "inference_model.add(Activation('softmax'))\n",
    "\n",
    "# Copy the weights of the trained network. Both should have the same exact number of parameters (why?).\n",
    "# inference_model.set_weights(model.get_weights())\n",
    "inference_model.load_weights('cocktail_weights.h5')\n",
    "\n",
    "# Given the start Character 'S' (one-hot encoded), predict the next most likely character.\n",
    "startChar = np.zeros((1, 1, len(char2id)))\n",
    "startChar[0, 0, char2id['S']] = 1\n",
    "nextCharProbabilities = inference_model.predict(startChar)\n",
    "\n",
    "# print the most probable character that goes next.\n",
    "print(id2char[nextCharProbabilities.argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'0', 0.63272119),\n",
       " (u'1', 0.19863351),\n",
       " (u'2', 0.073229872),\n",
       " (u'3', 0.02815249),\n",
       " (u'4', 0.017893698),\n",
       " (u'6', 0.010040422),\n",
       " (u'5', 0.00622067),\n",
       " (u'c', 0.0052369977),\n",
       " (u'i', 0.0042802417),\n",
       " (u'8', 0.0042548561)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charProbs = [(id2char[i], p) for i, p in enumerate(nextCharProbabilities.squeeze())]\n",
    "charProbs.sort(key=lambda i: i[1], reverse=True)\n",
    "charProbs[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sampling a Complete New Sentence\n",
    "Now that we have our inference_model working we can start producing new sentences by random sampling from the output of next character probabilities one step at a time. We rely on the np.random.multinomial function from numpy. To see what it does please check the documentation and make sure you understand what it does http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~\n",
      "0.188 cup coconut\n",
      "0.0933 teaspoon suparatens freshly squeezed orange\n",
      "juice from 1 darkmel torthes worcestershire sadges\n",
      "0.5 cans – 1/2 tablespoon taw splentted lemons\n",
      "0.0417.9 liters, tequila (optional)\n",
      "~~~~~\n",
      "0.0625 tba cold\n",
      "0.0909 cup flued sugar\n",
      "2 pans\n",
      "~~~~~\n",
      "0.25 lemons, sliced\n",
      "0.0833 cup grenadine\n",
      "1 cucumber cileron\n",
      "0.45 cups heavy\n",
      "ice\n",
      "750ml ripe betred seedseren oranges\n",
      "~~~~~\n",
      "3 risemmated lime od gomend, thins\n",
      "0.0835 tebspoon golden gin\n",
      "0.5 whote cloves\n",
      "0.0625 teaspoon sugar\n",
      "mint sprig medges, grened\n",
      "~~~~~\n",
      "1 tablespoons sugar\n",
      "0.5 tablespoons sugar\n",
      "0.25 lime wine\n",
      "2.809 ml triple sec\n",
      "0.5 tbs fresh wedges\n",
      "30 miderine syrup of sugar, freshly seatled heaveds in orckssmint sugar\n",
      "6 cup ili's sweetzes\n",
      "0.25 tbsp lemon juice\n",
      "1 singer temperries, juiced\n",
      "0.5 c%) water\n",
      "0.25 cardaninet lemons\n",
      "~~~~~\n",
      "0.429 cups ice cubes\n",
      "0.05 cups white sugar\n",
      "tequila\n",
      "1 cups tequila\n",
      "to taste\n",
      "10 teme driek\n",
      "0.5 tablespoons triple sec\n",
      "2 ounces puspey\n",
      "grewed lemon (sor gorgan asule mereato, chilled\n",
      "1 shots apple juice an juice fromm\n",
      "forshinish grougs\n",
      "0.125 ounce topberry top\n",
      "2.5 cups peppermint spices\n",
      "0.167 cups water\n",
      "0.0312 cup freshly straked (od us netuzer)\n",
      "0.167 teaspoon such as gele\n",
      "0.333 teaspoons freshly squeezed lime juice\n",
      "0.5 lemons z.wter pineapple juice\n",
      "0.167 bottle of lemon piel\n",
      "2.11 cup lome berries\n",
      "1 cups crushed ice\n",
      "lime slice\n",
      "0.75 cups ire sugar\n",
      "0.125 tablespoons sugar\n",
      "2 cups water\n",
      "0.5\n",
      "~~~~~\n",
      "1.5 ounces sugar\n",
      "0.0417 cup culacos\n",
      "0.125 cup peppers\n",
      "0.5 teaspoons tequila\n",
      "1 fluid ounce freshly squeezed lime juice\n",
      "2 lemons\n",
      "0.125 peach schnapps\n",
      "0.5 cup cold rum\n",
      "5 teaspoons water\n",
      "0.75 eggs, oumminof chupppins\n",
      "0.5 tablespoons grouncensic\n",
      "spicks fresh mint\n",
      "0.25 cups grapefruits\n",
      "strawberries\n",
      "4 ounces water\n",
      "0.0833 cup simple syrup\n",
      "0.025 bottle of spuck freshlybyor vodka\n",
      "0.56 dropberry peel cocheatice\n",
      "0.75 pince bott spices\n",
      "freshly vanillame syrasce abestuls\n",
      "0.5 dashes coffee\n",
      "0.188 oz raspberry juice\n",
      "0.0625 cup sugar\n",
      "0.5 fesplextragna or lime red pin\n",
      "~~~~~\n",
      "0.667 teaspoons welprise\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    inference_model.reset_states()  # This makes sure the initial hidden state is cleared every time.\n",
    "    startChar = np.zeros((1, 1, len(char2id)))\n",
    "    startChar[0, 0, char2id['S']] = 1\n",
    "    end = False\n",
    "    sent = \"\"\n",
    "    for i in range(0, maxSequenceLength):\n",
    "        nextCharProbs = inference_model.predict(startChar)\n",
    "\n",
    "        # In theory I should be able to input nextCharProbs to np.random.multinomial.\n",
    "        nextCharProbs = np.asarray(nextCharProbs).astype('float64') # Weird type cast issues if not doing this.\n",
    "        nextCharProbs = nextCharProbs / nextCharProbs.sum()  # Re-normalize for float64 to make exactly 1.0.\n",
    "\n",
    "        nextCharId = np.random.multinomial(1, nextCharProbs.squeeze(), 1).argmax()\n",
    "        if id2char[nextCharId] == 'E':\n",
    "            if not end:\n",
    "                print(\"~~~~~\")\n",
    "            end = True\n",
    "        else:\n",
    "            sent = sent + id2char[nextCharId] # The comma at the end avoids printing a return line character.\n",
    "        startChar.fill(0)\n",
    "        startChar[0, 0, nextCharId] = 1\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model learns to always predict 'E' once it has already predicted the first 'E' and does not produce any other character after that. In practice we can stop the for loop once we already found 'E', this has the effect of producing sentences of arbitrary size, meaning our model has learned when to finish a sentence. The sentence might not be perfect at this point in training but probably it has already learned to produce basic words like \"a\", \"the\", \"and\" or \"with\", however it still produces pseudo-words that look like words but are not actual words. Try running the above code many times, sentences will sound funny if you read them I guess. If you keep training the model for longer it should get better and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Questions (8 pts)\n",
    "0. In section 4, how long did it take for you to train an epoch on average? and how long did it take to train for 10 epochs? What was your hardware setup? (0.5pts)<br/>\n",
    "On average, 60s.  10 minutes for 10 epochs.  I'm running this on just a Intel 3570k CPU.<br/>\n",
    "1. In section 5 we predicted the next character after the starting character 'S' from the output probability distribution. Modify the code to print the top 10 most probable characters at the beginning of a sentence. Show the list of characters and their associated probability to show up as the first character in the sentence: (0.5pts)\n",
    "<br/>[('a', 0.7477212), ('t', 0.13135067), ('s', 0.020810049), ('p', 0.015685311), ('m', 0.014828731), ('b', 0.013467789), ('c', 0.0088301683), ('w', 0.0085571501), ('l', 0.005540017), ('g', 0.0045442022)]<br/>\n",
    "2. Print here a five sentences that you obtained from section 6 as a string (not as array and without 'E' characters). (0.5pts)\n",
    "    - a white soaking atha sitking in the his beachtor har it while counteroop\n",
    "    - a bathroom with a ehoolel herr sink a roor tolding andowarount and tomencoolatr\n",
    "    - ity it a kitchen with tur toderat niam holkightin biketaboden boardow\n",
    "    - a man roonsing a kitchen op sane\n",
    "    - a botellood red wark in a eroop opather asdelionts tabbeurarains\n",
    "    - a fould is howing a gooute if a stale\n",
    "    - a bathroom with o all wooden thiled with two fisture sinkind in a aroom\n",
    "    - towe topelan thoroor of athouboord bificels and in the silk\n",
    "    - a witchen with toilet beardom sink woile aroor fooploth\n",
    "    - a lasge wooden wooden aistwone filige stowite sink oidowhile ancer an aheas and a roop bike<br/>\n",
    "3. In section 6, what happens if you remove inference_model.reset_states() from the code? Try removing it and running section 6 code multiple times. Why do you get this effect? (0.5pts)<br/>When removing the inference state reset, the model assumes that the sentence is finished as it was before and thus keeps predicting 'E'.<br/>\n",
    "4. I have trained this model on a GPU for a few thousand epochs (until the loss went down to around 0.17) and obtained the following weight parameters: <a href=\"weights-vicente.hdf5\">weights-vicente.hdf5</a>. Try loading these weights into your model and producing five sentences (see load_weights in Keras). Include five senteces here as strings: (2pts)\n",
    "    - a bathroom with a glass darry proceinalllist appliances\n",
    "    - a nice kitchen with plated oblend laying inside a bagoox out on the floor\n",
    "    - a large commer table it and a pink shirt is like at a cemech\n",
    "    - a  gardage cookier down a street next to a train\n",
    "    - two colessued and paper tys most fisce entign cutting a large scall bowl<br/>\n",
    "5. Keras already includes an example of how to generate text character by character (using Nietzsche's writings as training text) here https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py. Please describe  what are the differences between that model and the model implemented in this lab. How are they different? (2pts)<br/>The model Keras uses predicts characters one at a time based on the past 40 words in the text (including the part of the word being predicted one character at a time).  This means that the input size is of size 40 x numChars.<br/>\n",
    "6. Include any thoughts that you have about what are other possible uses of this type of model. (For instance, instead of having a one-hot encoding vector for the starting character 'S' as your input you could have the output of a convolutional neural network from an image as the input -- this is the most popular model for generating image captions these days). (2pts) <br/>This type of model could be used to generate names (for people, pets, places...) one character at a time.  One could also have the output of some audio recognition network be used in the places of the 'S' character in order to generate text captions for spoken word.<br/>\n",
    "\n",
    "### Optional (2pts)\n",
    "\n",
    "1. Try to improve the model presented here by changing maybe batch_size, hiddenStateSize, hiddenLayerSize, adding a Dropout layer, Batch Normalization layer, etc. You could possibly obtain a very low loss function value much faster with the right combination.<br/><br/>\n",
    "2. Train the model in this lab using the Nietzche's writings from the Keras example on text generation (you might have to split that text into sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to lab questions (not inline version)\n",
    "\n",
    "\n",
    "1. On average, 60s.  10 minutes for 10 epochs.  I'm running this on just a Intel 3570k CPU.\n",
    "2. [('a', 0.7477212), ('t', 0.13135067), ('s', 0.020810049), ('p', 0.015685311), ('m', 0.014828731), ('b', 0.013467789), ('c', 0.0088301683), ('w', 0.0085571501), ('l', 0.005540017), ('g', 0.0045442022)]\n",
    "3. sentences below:\n",
    "    - a white soaking atha sitking in the his beachtor har it while counteroop\n",
    "    - a bathroom with a ehoolel herr sink a roor tolding andowarount and tomencoolatr\n",
    "    - ity it a kitchen with tur toderat niam holkightin biketaboden boardow\n",
    "    - a man roonsing a kitchen op sane\n",
    "    - a botellood red wark in a eroop opather asdelionts tabbeurarains\n",
    "    - a fould is howing a gooute if a stale\n",
    "    - a bathroom with o all wooden thiled with two fisture sinkind in a aroom\n",
    "    - towe topelan thoroor of athouboord bificels and in the silk\n",
    "    - a witchen with toilet beardom sink woile aroor fooploth\n",
    "    - a lasge wooden wooden aistwone filige stowite sink oidowhile ancer an aheas and a roop bike\n",
    "\n",
    "4. When removing the inference state reset, the model assumes that the sentence is finished as it was before and thus keeps predicting 'E'.\n",
    "5. sentences below:\n",
    "    - a bathroom with a glass darry proceinalllist appliances\n",
    "    - a nice kitchen with plated oblend laying inside a bagoox out on the floor\n",
    "    - a large commer table it and a pink shirt is like at a cemech\n",
    "    - a  gardage cookier down a street next to a train\n",
    "    - two colessued and paper tys most fisce entign cutting a large scall bowl\n",
    "6. The model Keras uses predicts characters one at a time based on the past 40 words in the text (including the part of the word being predicted one character at a time).  This means that the input size is of size 40 x numChars.\n",
    "7. This type of model could be used to generate names (for people, pets, places...) one character at a time.  One could also have the output of some audio recognition network be used in the places of the 'S' character in order to generate text captions for spoken word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:0.8em;color:#888;text-align:center;padding-top:20px;\">If you find any errors or omissions in this material please contact me at vicente@cs.virginia.edu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

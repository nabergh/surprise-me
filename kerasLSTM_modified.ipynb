{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "In this lab we will experiment with recurrent neural networks. These are a useful type of model for predicting sequences or handling sequences of things as inputs. We will implement them in Keras+Tensorflow but many implementations can be found online with many sets of variants. Here are installation instructions for Keras: https://keras.io/#installation, and here are installation instructions for Tensorflow: https://github.com/tensorflow/tensorflow#download-and-setup. You should also be able to run those from a Docker container.\n",
    "\n",
    "We will take a set of 10 thousand image descriptions from the MS-COCO dataset (400,000 sentences) and make our recurrent network learn how to compose new sentences character by character.\n",
    "You can download this data here: http://www.cs.virginia.edu/~vicente/recognition/captions_train.txt.zip\n",
    "\n",
    "First, let's import libraries and make sure you have everything properly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import random\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing the Text\n",
    "We will first read the sentences and map each character to a unique identifier so that we can treat each sentence as an array of character ids. The code below loads the captions from a text file and places them inside a caption tensor that is a matrix of size numCaptions x maxCaptionLength x charVocabularySize. We will also create a caption tensor that contains the sentences but shifted by one character. Each character is mapped to an incremental ID, so we keep two hashmaps to convert from character to id and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "501\n",
      "51\n"
     ]
    }
   ],
   "source": [
    "all_recipes = pickle.load(open('dataset/cleaned_recipes.p', 'rb'))\n",
    "char2id = pickle.load(open('dataset/char2id.p', 'rb'))\n",
    "id2char = pickle.load(open('dataset/id2char.p', 'rb'))\n",
    "max_recipe_length = 500\n",
    "max_name_length = 50\n",
    "maxSequenceLength = 501\n",
    "maxNameSequenceLength = 51\n",
    "\n",
    "def booze_permuter(recipes):\n",
    "    while True:\n",
    "        for rec in recipes:\n",
    "            ings = rec['ingredients']\n",
    "            i = 0\n",
    "            while i < len(ings) and is_number(ings[i][0]):\n",
    "                i += 1\n",
    "\n",
    "            ing_list = ings[:i]\n",
    "            garn_list = ings[i:]\n",
    "\n",
    "            random.shuffle(ing_list)\n",
    "            random.shuffle(garn_list)\n",
    "\n",
    "            ing_list.extend(garn_list)\n",
    "            yield '\\n'.join(ing_list)\n",
    "        \n",
    "def training_set_generator(num_recipes):\n",
    "    recipe_generator = booze_permuter(all_recipes)\n",
    "    while True:\n",
    "        recipe_list = []\n",
    "        while len(recipe_list) < num_recipes:\n",
    "            recipe_list.append(next(recipe_generator))\n",
    "            \n",
    "        maxSequenceLength = max_recipe_length + 1\n",
    "        inputChars = np.zeros((len(recipe_list), maxSequenceLength, len(char2id)), dtype=np.bool)\n",
    "        nextChars = np.zeros((len(recipe_list), maxSequenceLength, len(char2id)), dtype=np.bool)\n",
    "\n",
    "        for i in range(0, len(recipe_list)):\n",
    "            inputChars[i, 0, char2id['S']] = 1\n",
    "            nextChars[i, 0, char2id[recipe_list[i][0]]] = 1\n",
    "            for j in range(1, maxSequenceLength):\n",
    "                if j < len(recipe_list[i]) + 1:\n",
    "                    inputChars[i, j, char2id[recipe_list[i][j - 1]]] = 1\n",
    "                    if j < len(recipe_list[i]):\n",
    "                        nextChars[i, j, char2id[recipe_list[i][j]]] = 1\n",
    "                    else:\n",
    "                        nextChars[i, j, char2id['E']] = 1\n",
    "                else:\n",
    "                    inputChars[i, j, char2id['E']] = 1\n",
    "                    nextChars[i, j, char2id['E']] = 1\n",
    "        \n",
    "        yield (inputChars, nextChars)\n",
    "\n",
    "\n",
    "\n",
    "def name_booze_permuter(recipes):\n",
    "    while True:\n",
    "        for rec in recipes:\n",
    "            if len(rec['name']) <= max_name_length:\n",
    "                ings = rec['ingredients']\n",
    "                i = 0\n",
    "                while i < len(ings) and is_number(ings[i][0]):\n",
    "                    i += 1\n",
    "\n",
    "                ing_list = ings[:i]\n",
    "                garn_list = ings[i:]\n",
    "\n",
    "                random.shuffle(ing_list)\n",
    "                random.shuffle(garn_list)\n",
    "\n",
    "                ing_list.extend(garn_list)\n",
    "                yield ('\\n'.join(ing_list), rec['name'].lower())\n",
    "\n",
    "def name_training_set_generator(num_recipes):\n",
    "    recipe_generator = name_booze_permuter(all_recipes)\n",
    "    while True:\n",
    "        recipe_list = []\n",
    "        name_list = []\n",
    "        while len(recipe_list) < num_recipes:\n",
    "            example = next(recipe_generator)\n",
    "            recipe_list.append(example[0])\n",
    "            name_list.append(example[1])\n",
    "            \n",
    "        maxSequenceLength = max_recipe_length + 1\n",
    "        maxNameSequenceLength = max_name_length + 1\n",
    "\n",
    "        recipeChars = np.zeros((num_recipes, maxSequenceLength, len(char2id)), dtype=np.bool)\n",
    "        inNameChars = np.zeros((num_recipes, maxNameSequenceLength, len(char2id)), dtype=np.bool)\n",
    "        nextNameChars = np.zeros((num_recipes, maxNameSequenceLength, len(char2id)), dtype=np.bool)\n",
    "\n",
    "        for i in range(0, num_recipes):\n",
    "            recipeChars[i, 0, char2id['S']] = 1\n",
    "            nextNameChars[i, 0, char2id[name_list[i][0]]] = 1\n",
    "            for j in range(1, maxSequenceLength):\n",
    "                if j < len(recipe_list[i]) + 1:\n",
    "                    recipeChars[i, j, char2id[recipe_list[i][j - 1]]] = 1\n",
    "                else:\n",
    "                    recipeChars[i, j, char2id['E']] = 1\n",
    "            inNameChars[i, 0, char2id['S']] = 1\n",
    "            for j in range(1, maxNameSequenceLength):\n",
    "                if j <= len(name_list[i]):\n",
    "                    if name_list[i][j - 1] not in char2id:\n",
    "                        inNameChars[i, j, char2id[' ']] = 1\n",
    "                    else:\n",
    "                        inNameChars[i, j, char2id[name_list[i][j - 1]]] = 1\n",
    "                        \n",
    "                    if j < len(name_list[i]):\n",
    "                        if name_list[i][j] not in char2id:\n",
    "                            nextNameChars[i, j, char2id[' ']] = 1\n",
    "                        else:\n",
    "                            nextNameChars[i, j, char2id[name_list[i][j]]] = 1\n",
    "                    else:\n",
    "                        nextNameChars[i, j, char2id['E']] = 1\n",
    "                        \n",
    "                else:\n",
    "                    inNameChars[i, j, char2id['E']] = 1\n",
    "                    nextNameChars[i, j, char2id['E']] = 1\n",
    "        yield ([recipeChars, inNameChars], nextNameChars)\n",
    "\n",
    "        \n",
    "gen = name_training_set_generator(128)\n",
    "inputs, nextChars = next(gen)\n",
    "\n",
    "print(len(inputs))\n",
    "print(len(inputs[0][0]))\n",
    "print(len(inputs[1][0]))\n",
    "\n",
    "# Compute a char2id and id2char vocabulary.\n",
    "# test_set = get_new_recipes()\n",
    "# charIndex = 0\n",
    "# for recipe in test_set:\n",
    "#     for char in recipe:\n",
    "#         if char not in char2id:\n",
    "#             char2id[char] = charIndex\n",
    "#             id2char[charIndex] = char\n",
    "#             charIndex += 1\n",
    "\n",
    "# # Add a special starting and ending character to the dictionary.\n",
    "# char2id['S'] = charIndex; id2char[charIndex] = 'S'  # Special sentence start character.\n",
    "# char2id['E'] = charIndex + 1; id2char[charIndex + 1] = 'E'  # Special sentence ending character.\n",
    "# pickle.dump(char2id, open('char2id_2.p', 'wb'), protocol = 2)\n",
    "# pickle.dump(id2char, open('id2char_2.p', 'wb'), protocol = 2)\n",
    "# pickle.dump(all_recipes, open('cleaned_recipes_2.p', 'wb'), protocol = 2)\n",
    "\n",
    "\n",
    "\n",
    "# print(\"input:\")\n",
    "# print(inputChars.shape)  # Print the size of the inputCharacters tensor.\n",
    "# print(\"output:\")\n",
    "# print(nextChars.shape)  # Print the size of the nextCharacters tensor.\n",
    "# print(\"char2id:\")\n",
    "# print(char2id)  # Print the character to ids mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<b>Note:</b> In order to clearly show how inputChars, and nextChars store the sequences, let's try printing a sentence back from its stored format in these two arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S' 'b' 'o' 'o' 'z' 'y' ' ' 'c' 'r' 'a' 'n' 'b' 'e' 'r' 'r' 'y' ' ' 'p'\n",
      " 'u' 'n' 'c' 'h' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n",
      "['b' 'o' 'o' 'z' 'y' ' ' 'c' 'r' 'a' 'n' 'b' 'e' 'r' 'r' 'y' ' ' 'p' 'u'\n",
      " 'n' 'c' 'h' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E'\n",
      " 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E' 'E']\n"
     ]
    }
   ],
   "source": [
    "inputChars = inputs[1]\n",
    "trainCaption = inputChars[25, :, :]  # Pick some caption\n",
    "labelCaption = nextChars[25, :, :]  # Pick what we are trying to predict.\n",
    "\n",
    "def printCaption(sampleCaption):\n",
    "    charIds = np.zeros(sampleCaption.shape[0])\n",
    "    for (idx, elem) in enumerate(sampleCaption):\n",
    "#         print(np.nonzero(elem))\n",
    "\n",
    "        charIds[idx] = np.nonzero(elem)[0].squeeze()\n",
    "    print(np.array([id2char[x] for x in charIds]))\n",
    "\n",
    "printCaption(trainCaption)\n",
    "printCaption(labelCaption)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output, you will notice that the sentences are indeed shifted. This is because we are going to predict the next character at each timestep. The first character is 'S' which means start of sentences, and the next character in our target should be 'a' which is the first actual character of the sentence. The later characters in the sentence will also use the \"history\" of all previous characters to find out what goes next. \n",
    "\n",
    "## 2. Building our model using an LSTM Recurrent Network.\n",
    "Next we will create a recurrent neural network using Keras which takes an input set of characters (one-hot encoded) of size (batch_size, maxSequenceLength, charVocabularySize), similarly the output of this network will be a vector of size (batch_size, maxSequenceLength, charVocabularySize). However, the output does not contain one-hot encodings. The output contains a probability distribution (the output of a softmax) for every time step in the sequence. We see in section 4 how to decode the sequence from this distribution, you can just take the character corresponding to the index with the max probability for every time step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building training model...\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lstm_1 (LSTM)                    (None, 501, 128)      131584      lstm_input_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribute(None, 501, 128)      16512       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribute(None, 501, 128)      0           timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribute(None, 501, 128)      16512       timedistributed_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_4 (TimeDistribute(None, 501, 128)      0           timedistributed_3[0][0]          \n",
      "====================================================================================================\n",
      "Total params: 164608\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Sample input Batch size:\n",
      "(32, 501, 128)\n",
      "Sample input Batch labels (nextChars):\n",
      "(32, 501, 128)\n",
      "Output Sequence size:\n",
      "(32, 501, 128)\n"
     ]
    }
   ],
   "source": [
    "print('Building training model...')\n",
    "hiddenStateSize = 128\n",
    "hiddenLayerSize = 128\n",
    "model = Sequential()\n",
    "# The output of the LSTM layer are the hidden states of the LSTM for every time step. \n",
    "model.add(LSTM(hiddenStateSize, return_sequences = True, input_shape=(maxSequenceLength, len(char2id))))\n",
    "# Two things to notice here:\n",
    "# 1. The Dense Layer is equivalent to nn.Linear(hiddenStateSize, hiddenLayerSize) in Torch.\n",
    "#    In Keras, we often do not need to specify the input size of the layer because it gets inferred for us.\n",
    "# 2. TimeDistributed applies the linear transformation from the Dense layer to every time step\n",
    "#    of the output of the sequence produced by the LSTM.\n",
    "model.add(TimeDistributed(Dense(hiddenLayerSize)))\n",
    "model.add(TimeDistributed(Activation('relu'))) \n",
    "model.add(TimeDistributed(Dense(len(char2id))))  # Add another dense layer with the desired output size.\n",
    "model.add(TimeDistributed(Activation('softmax')))\n",
    "# We also specify here the optimization we will use, in this case we use RMSprop with learning rate 0.001.\n",
    "# RMSprop is commonly used for RNNs instead of regular SGD.\n",
    "# See this blog for info on RMSprop (http://sebastianruder.com/optimizing-gradient-descent/index.html#rmsprop)\n",
    "# categorical_crossentropy is the same loss used for classification problems using softmax. (nn.ClassNLLCriterion)\n",
    "model.compile(loss='categorical_crossentropy', optimizer = RMSprop(lr=0.001))\n",
    "\n",
    "print(model.summary()) # Convenient function to see details about the network model.\n",
    "\n",
    "# Test a simple prediction on a batch for this model.\n",
    "print(\"Sample input Batch size:\"),\n",
    "print(inputChars[0:32, :, :].shape)\n",
    "print(\"Sample input Batch labels (nextChars):\"),\n",
    "print(nextChars[0:32, :, :].shape)\n",
    "outputs = model.predict(inputChars[0:32, :, :])\n",
    "print(\"Output Sequence size:\"),\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training the Model\n",
    "Keras already implements a generic trainModel functionality through the model.fit function, but it also contains model.train_on_batch if you want to perform the training for loop yourself. For more informations about Keras model functionalities you can see here: https://keras.io/models/model/\n",
    "\n",
    "If you installed Tensorflow with GPU support, this will automatically run on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "30724/30724 [==============================] - 650s - loss: 1.1430   \n",
      "Epoch 2/10\n",
      "30724/30724 [==============================] - 650s - loss: 0.6997   \n",
      "Epoch 3/10\n",
      "30724/30724 [==============================] - 637s - loss: 0.5498   \n",
      "Epoch 4/10\n",
      "30724/30724 [==============================] - 629s - loss: 0.4638   \n",
      "Epoch 5/10\n",
      "30724/30724 [==============================] - 629s - loss: 0.4070   \n",
      "Epoch 6/10\n",
      "30724/30724 [==============================] - 674s - loss: 0.3692   \n",
      "Epoch 7/10\n",
      "30724/30724 [==============================] - 803s - loss: 0.3432   \n",
      "Epoch 8/10\n",
      "30724/30724 [==============================] - 832s - loss: 0.3249   \n",
      "Epoch 9/10\n",
      "30724/30724 [==============================] - 830s - loss: 0.3115   \n",
      "Epoch 10/10\n",
      "30724/30724 [==============================] - 802s - loss: 0.3011   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd0c9272a50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(inputChars, nextChars, batch_size = 128, nb_epoch = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verifying the Model is indeed Learning\n",
    "Here we input an arbitrary caption from the training set (one-hot encoded), compute the output using the trained model, and decode this output back into a char array. Ideally we should see the same input caption shifted by one character. However you would need to run the training code for around 24 hours straight to get the model close to that point (it is ok if you only run the model for 10 iterations for the purposes of this lab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model.save_weights('cocktail_weights.h5')\n",
    "model.load_weights('cocktail_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input Batch size:\n",
      "(32, 501, 128)\n",
      "Sample input Batch labels (nextChars):\n",
      "(32, 501, 128)\n",
      "Output Sequence size:\n",
      "(32, 501, 128)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-69879bff9277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# printCaption(inputCaption[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "print(\"Sample input Batch size:\"),\n",
    "print(inputChars[0:32, :, :].shape)\n",
    "print(\"Sample input Batch labels (nextChars):\"),\n",
    "print(nextChars[0:32, :, :].shape)\n",
    "outputs = model.predict(inputChars[0:32, :, :])\n",
    "print(\"Output Sequence size:\"),\n",
    "print(outputs.shape)\n",
    "# Test a simple prediction on a batch for this model.\n",
    "captionId = 132\n",
    "\n",
    "inputCaption = inputChars[captionId:captionId+1, :, :]\n",
    "outputs = model.predict(inputCaption)\n",
    "# printCaption(inputCaption[0])\n",
    "print(''.join([id2char[x.argmax()] for x in outputs[0, :, :]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building the Inference Model.\n",
    "We verified in the previous section that the model was somewhat working on training data. However, we want to be able to create new sentences from this model starting from zero. We want to use the same parameters of the trained model to produce text character by character. We build here such model and just copy the parameters from our trained model above. We show in the following section (section 6) how to produce the sentences using this inference_model. Please pay attention to all the comments in the code below to see what are the differences with the model at training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Inference model...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# The only difference with the \"training model\" is that here the input sequence has \n",
    "# a length of one because we will predict character by character.\n",
    "print('Building Inference model...')\n",
    "inference_model = Sequential()\n",
    "# Two differences here.\n",
    "# 1. The inference model only takes one sample in the batch, and it always has sequence length 1.\n",
    "# 2. The inference model is stateful, meaning it inputs the output hidden state (\"its history state\")\n",
    "#    to the next batch input.\n",
    "inference_model.add(LSTM(hiddenStateSize, batch_input_shape=(1, 1, len(char2id)), stateful = True))\n",
    "# Since the above LSTM does not output sequences, we don't need TimeDistributed anymore.\n",
    "inference_model.add(Dense(hiddenLayerSize))\n",
    "inference_model.add(Activation('relu'))\n",
    "inference_model.add(Dense(len(char2id)))\n",
    "inference_model.add(Activation('softmax'))\n",
    "\n",
    "# Copy the weights of the trained network. Both should have the same exact number of parameters (why?).\n",
    "# inference_model.set_weights(model.get_weights())\n",
    "inference_model.load_weights('gpu_weights.h5')\n",
    "\n",
    "# Given the start Character 'S' (one-hot encoded), predict the next most likely character.\n",
    "startChar = np.zeros((1, 1, len(char2id)))\n",
    "startChar[0, 0, char2id['S']] = 1\n",
    "nextCharProbabilities = inference_model.predict(startChar)\n",
    "\n",
    "# print the most probable character that goes next.\n",
    "print(id2char[nextCharProbabilities.argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0', 0.59262323),\n",
       " ('1', 0.22587761),\n",
       " ('2', 0.075185753),\n",
       " ('3', 0.031022567),\n",
       " ('4', 0.018277779),\n",
       " ('6', 0.012529056),\n",
       " ('5', 0.0079699028),\n",
       " ('8', 0.0037487082),\n",
       " ('i', 0.0035882797),\n",
       " ('c', 0.0034658809)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charProbs = [(id2char[i], p) for i, p in enumerate(nextCharProbabilities.squeeze())]\n",
    "charProbs.sort(key=lambda i: i[1], reverse=True)\n",
    "charProbs[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sampling a Complete New Sentence\n",
    "Now that we have our inference_model working we can start producing new sentences by random sampling from the output of next character probabilities one step at a time. We rely on the np.random.multinomial function from numpy. To see what it does please check the documentation and make sure you understand what it does http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.multinomial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '.', 2: '5', 3: ' ', 4: 'o', 5: 'u', 6: 'n', 7: 'c', 8: 'e', 9: 's', 10: 'w', 11: 'h', 12: 'i', 13: 'k', 14: 'y', 15: '\\n', 16: '1', 17: 'g', 18: 'r', 19: 'b', 20: 'p', 21: 'm', 22: 'a', 23: 't', 24: 'j', 25: 'd', 26: '(', 27: 'l', 28: ',', 29: 'f', 30: ')', 31: 'z', 32: 'v', 33: 'q', 34: '4', 35: '2', 36: '6', 37: \"'\", 38: '&', 39: '3', 40: '%', 41: 'x', 42: '7', 43: '”', 44: '!', 45: '½', 46: '’', 47: '8', 48: ':', 49: '®', 50: '/', 51: '9', 52: '+', 53: '*', 54: '–', 55: '™', 56: 'é', 57: '⅓', 58: 'ñ', 59: 'ç', 60: ';', 61: '~', 62: '¼', 63: 'ó', 64: '[', 65: ']', 66: '\"', 67: '>', 68: 'è', 69: '•', 70: '{', 71: '}', 72: 'ú', 73: '⁄', 74: 'í', 75: 'ä', 76: '“', 77: 'á', 78: '‘', 79: '=', 80: '?', 81: '#', 82: '¾', 83: '·', 84: '|', 85: '…', 86: '\\\\', 87: '—', 88: 'ë', 89: '″', 90: 'ü', 91: '©', 92: 'ö', 93: 'â', 94: '¹', 95: '³', 96: '²', 97: '⅝', 98: '°', 99: '$', 100: '̀', 101: '∕', 102: 'ê', 103: 'à', 104: 'û', 105: '@', 106: 'ﬂ', 107: '⅔', 108: 'ż', 109: 'ô', 110: '⅛', 111: '●', 112: '�', 113: '̃', 114: '♥', 115: 'ã', 116: '±', 117: '\\x96', 118: '<', 119: '×', 120: '_', 121: 'ō', 122: 'º', 123: 'î', 124: '❤', 125: 'ß', 126: 'S', 127: 'E'}\n",
      "{'#': 81, '·': 83, 'b': 19, '[': 64, 'n': 6, '°': 98, 'ä': 75, '?': 80, '½': 45, '²': 96, '™': 55, ']': 65, '̃': 113, '…': 85, '6': 36, 'r': 18, 'ō': 121, '&': 38, '¹': 94, '©': 91, '⅓': 57, '>': 67, 'd': 25, '0': 0, '—': 87, '¾': 82, ',': 28, 't': 23, 'x': 41, '=': 79, '_': 120, 'v': 32, '(': 26, 'q': 33, 'ë': 88, '–': 54, '“': 76, '.': 1, '\\n': 15, 'ż': 108, '7': 42, 'u': 5, 'w': 10, '+': 52, '♥': 114, 'í': 74, '1': 16, 'y': 14, 'E': 127, 's': 9, '×': 119, 'ö': 92, '³': 95, 'k': 13, '/': 50, 'â': 93, 'S': 126, '@': 105, 'ã': 115, '~': 61, '}': 71, \"'\": 37, '5': 2, '9': 51, 'f': 29, '!': 44, 'ü': 90, '$': 99, '⁄': 73, 'p': 20, 'ú': 72, '{': 70, '3': 39, 'h': 11, 'ó': 63, '*': 53, '|': 84, '±': 116, ';': 60, '⅔': 107, 'a': 22, '4': 34, 'z': 31, 'g': 17, 'ﬂ': 106, '∕': 101, 'l': 27, 'à': 103, 'è': 68, 'î': 123, 'ñ': 58, '”': 43, 'á': 77, '⅛': 110, '\\x96': 117, 'ß': 125, 'i': 12, 'ê': 102, '8': 47, '●': 111, '″': 89, 'm': 21, '\"': 66, '%': 40, ':': 48, '̀': 100, '¼': 62, 'ô': 109, '•': 69, 'c': 7, '<': 118, '⅝': 97, '®': 49, 'j': 24, '2': 35, 'e': 8, 'é': 56, '‘': 78, 'ç': 59, ' ': 3, '❤': 124, 'o': 4, 'º': 122, ')': 30, 'û': 104, '\\\\': 86, '’': 46, '�': 112}\n",
      "~~~~~\n",
      "0.25 fresh piece piesh gelamin\n",
      "0.25 teaspoon fresh lime juice\n",
      "1 to 3 ounces rum\n",
      "0.75 ounce gin\n",
      "0.25 ounce dry white wine\n",
      "1.25 cups ice (prow chunk)\n",
      "2 tablespoons vodka\n",
      "~~~~~\n",
      "0.15 orange peel\n",
      "6.92 vanilla pod extra (optional) fresh mint leaves\n",
      "0.0833 cup lemon juice, freencet or 1 orange, slice\n",
      "~~~~~\n",
      "0.25 fluid ounce simple syrup\n",
      "0.75 ounce orange liqueur\n",
      "ice\n",
      "1.25 fluid ounce vodka\n",
      "rose: to® bitter (marginia)\n",
      "1 ounce rum\n",
      "0.25 ounce triple sec\n",
      "0.75 ounce vodka\n",
      "0.5 ounce creme de barant vodka (i 3 i or 443.7 partsubs ejis beer)\n",
      "1.33 ounce vodka\n",
      "0.25 ounce cointreau liqueur, eqmalt , pitany puree and peeled and pith egg white\n",
      "0.625 ounce amaretto liqueur\n",
      "0.0833 cup sugar\n",
      "0.104 ginger, heavy cream, drink\n",
      "0.125 oz grand marnier\n",
      "0.5 wedges tomato wine, chilled\n",
      "0.375 oz. fresh lime juice (plus extra)\n",
      "cinnamon (or peenerada nota tablecoff iz maprional cordial\n",
      "1 ounce vodka\n",
      "1.5 whole milk\n",
      "0.75 ounce or juice of morinark brown sugar\n",
      "0.5 cup sugar (or a cant ano.55 itolians sweet brawb. or ing white pert water)\n",
      "cinnamon (such as cointreau or green lemonade iptional, of parro earlion)\n",
      "0.5 ounce lime juice\n",
      "0.75 ounce fresh lime juice\n",
      "0.125 part tequila or rye white wine\n",
      "crushed peppirate (cherries of your cup ir 12 to 4 beef) 4 1/2 ounces crushed ice (optional)\n",
      "0.5 ounce gin (recommen below)\n",
      "0.5 cup water\n",
      "1.5 lime wedge\n",
      "1.5 ounces coconut rum (optional vod\n",
      "~~~~~\n",
      "0.25 oz raspberry liqueur\n",
      "1.25 tablespoons fresh lime juice\n",
      "0.0625 teaspoon powdered sugar\n",
      "0.188 ounce orange juice\n",
      "~~~~~\n",
      "0.125 teaspoon tea bags\n",
      "~~~~~\n",
      "0.25 ounce fresh lime juice\n",
      "15 ml apple schnapps\n",
      "~~~~~\n",
      "0.25 pint a frozen bottle dry vost cream\n",
      "0.167 teaspoon grated nutmeg\n",
      "0.375 teaspoons grand marnier\n",
      "0.0833 teaspoon bitters\n",
      "0.345 cups fresh lime juice\n",
      "0.0352 teaspoon crystallized ginger beer\n",
      "1 orange white fresh glass\n",
      "0.0417 teaspoon lemon juice\n",
      "lothtarvy or amaretto powder\n",
      "0.244 liter® buttershize (confectros such as temoned to sprite pint more drink or moscato arite\n",
      "0.25 oz) cans pineapple juice\n",
      "~~~~~\n",
      "2 dashes gel citrusm vinegar\n",
      "0.5 ounce bailey's irish cream\n",
      "0.5 ounce cranberry juice\n",
      "sel sugar top pepsirg to taste\n",
      "ice cube\n",
      "2 ounces cointreau or wine\n",
      "3 ounces white wine\n",
      "0.5 ounce cream\n",
      "1 ounce pineapple rum\n",
      "1 pineapple juice from 2 lime\n"
     ]
    }
   ],
   "source": [
    "print(id2char)\n",
    "print(char2id)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    inference_model.reset_states()  # This makes sure the initial hidden state is cleared every time.\n",
    "    startChar = np.zeros((1, 1, len(char2id)))\n",
    "    startChar[0, 0, char2id['S']] = 1\n",
    "    end = False\n",
    "    sent = \"\"\n",
    "    for i in range(0, maxSequenceLength):\n",
    "        nextCharProbs = inference_model.predict(startChar)\n",
    "\n",
    "        # In theory I should be able to input nextCharProbs to np.random.multinomial.\n",
    "        nextCharProbs = np.asarray(nextCharProbs).astype('float64') # Weird type cast issues if not doing this.\n",
    "        nextCharProbs = nextCharProbs / nextCharProbs.sum()  # Re-normalize for float64 to make exactly 1.0.\n",
    "\n",
    "        nextCharId = np.random.multinomial(1, nextCharProbs.squeeze(), 1).argmax()\n",
    "        if id2char[nextCharId] == 'E':\n",
    "            if not end:\n",
    "                print(\"~~~~~\")\n",
    "            end = True\n",
    "        else:\n",
    "            sent = sent + id2char[nextCharId] # The comma at the end avoids printing a return line character.\n",
    "        startChar.fill(0)\n",
    "        startChar[0, 0, nextCharId] = 1\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the model learns to always predict 'E' once it has already predicted the first 'E' and does not produce any other character after that. In practice we can stop the for loop once we already found 'E', this has the effect of producing sentences of arbitrary size, meaning our model has learned when to finish a sentence. The sentence might not be perfect at this point in training but probably it has already learned to produce basic words like \"a\", \"the\", \"and\" or \"with\", however it still produces pseudo-words that look like words but are not actual words. Try running the above code many times, sentences will sound funny if you read them I guess. If you keep training the model for longer it should get better and better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Questions (8 pts)\n",
    "0. In section 4, how long did it take for you to train an epoch on average? and how long did it take to train for 10 epochs? What was your hardware setup? (0.5pts)<br/>\n",
    "On average, 60s.  10 minutes for 10 epochs.  I'm running this on just a Intel 3570k CPU.<br/>\n",
    "1. In section 5 we predicted the next character after the starting character 'S' from the output probability distribution. Modify the code to print the top 10 most probable characters at the beginning of a sentence. Show the list of characters and their associated probability to show up as the first character in the sentence: (0.5pts)\n",
    "<br/>[('a', 0.7477212), ('t', 0.13135067), ('s', 0.020810049), ('p', 0.015685311), ('m', 0.014828731), ('b', 0.013467789), ('c', 0.0088301683), ('w', 0.0085571501), ('l', 0.005540017), ('g', 0.0045442022)]<br/>\n",
    "2. Print here a five sentences that you obtained from section 6 as a string (not as array and without 'E' characters). (0.5pts)\n",
    "    - a white soaking atha sitking in the his beachtor har it while counteroop\n",
    "    - a bathroom with a ehoolel herr sink a roor tolding andowarount and tomencoolatr\n",
    "    - ity it a kitchen with tur toderat niam holkightin biketaboden boardow\n",
    "    - a man roonsing a kitchen op sane\n",
    "    - a botellood red wark in a eroop opather asdelionts tabbeurarains\n",
    "    - a fould is howing a gooute if a stale\n",
    "    - a bathroom with o all wooden thiled with two fisture sinkind in a aroom\n",
    "    - towe topelan thoroor of athouboord bificels and in the silk\n",
    "    - a witchen with toilet beardom sink woile aroor fooploth\n",
    "    - a lasge wooden wooden aistwone filige stowite sink oidowhile ancer an aheas and a roop bike<br/>\n",
    "3. In section 6, what happens if you remove inference_model.reset_states() from the code? Try removing it and running section 6 code multiple times. Why do you get this effect? (0.5pts)<br/>When removing the inference state reset, the model assumes that the sentence is finished as it was before and thus keeps predicting 'E'.<br/>\n",
    "4. I have trained this model on a GPU for a few thousand epochs (until the loss went down to around 0.17) and obtained the following weight parameters: <a href=\"weights-vicente.hdf5\">weights-vicente.hdf5</a>. Try loading these weights into your model and producing five sentences (see load_weights in Keras). Include five senteces here as strings: (2pts)\n",
    "    - a bathroom with a glass darry proceinalllist appliances\n",
    "    - a nice kitchen with plated oblend laying inside a bagoox out on the floor\n",
    "    - a large commer table it and a pink shirt is like at a cemech\n",
    "    - a  gardage cookier down a street next to a train\n",
    "    - two colessued and paper tys most fisce entign cutting a large scall bowl<br/>\n",
    "5. Keras already includes an example of how to generate text character by character (using Nietzsche's writings as training text) here https://github.com/fchollet/keras/blob/master/examples/lstm_text_generation.py. Please describe  what are the differences between that model and the model implemented in this lab. How are they different? (2pts)<br/>The model Keras uses predicts characters one at a time based on the past 40 words in the text (including the part of the word being predicted one character at a time).  This means that the input size is of size 40 x numChars.<br/>\n",
    "6. Include any thoughts that you have about what are other possible uses of this type of model. (For instance, instead of having a one-hot encoding vector for the starting character 'S' as your input you could have the output of a convolutional neural network from an image as the input -- this is the most popular model for generating image captions these days). (2pts) <br/>This type of model could be used to generate names (for people, pets, places...) one character at a time.  One could also have the output of some audio recognition network be used in the places of the 'S' character in order to generate text captions for spoken word.<br/>\n",
    "\n",
    "### Optional (2pts)\n",
    "\n",
    "1. Try to improve the model presented here by changing maybe batch_size, hiddenStateSize, hiddenLayerSize, adding a Dropout layer, Batch Normalization layer, etc. You could possibly obtain a very low loss function value much faster with the right combination.<br/><br/>\n",
    "2. Train the model in this lab using the Nietzche's writings from the Keras example on text generation (you might have to split that text into sentences)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers to lab questions (not inline version)\n",
    "\n",
    "\n",
    "1. On average, 60s.  10 minutes for 10 epochs.  I'm running this on just a Intel 3570k CPU.\n",
    "2. [('a', 0.7477212), ('t', 0.13135067), ('s', 0.020810049), ('p', 0.015685311), ('m', 0.014828731), ('b', 0.013467789), ('c', 0.0088301683), ('w', 0.0085571501), ('l', 0.005540017), ('g', 0.0045442022)]\n",
    "3. sentences below:\n",
    "    - a white soaking atha sitking in the his beachtor har it while counteroop\n",
    "    - a bathroom with a ehoolel herr sink a roor tolding andowarount and tomencoolatr\n",
    "    - ity it a kitchen with tur toderat niam holkightin biketaboden boardow\n",
    "    - a man roonsing a kitchen op sane\n",
    "    - a botellood red wark in a eroop opather asdelionts tabbeurarains\n",
    "    - a fould is howing a gooute if a stale\n",
    "    - a bathroom with o all wooden thiled with two fisture sinkind in a aroom\n",
    "    - towe topelan thoroor of athouboord bificels and in the silk\n",
    "    - a witchen with toilet beardom sink woile aroor fooploth\n",
    "    - a lasge wooden wooden aistwone filige stowite sink oidowhile ancer an aheas and a roop bike\n",
    "\n",
    "4. When removing the inference state reset, the model assumes that the sentence is finished as it was before and thus keeps predicting 'E'.\n",
    "5. sentences below:\n",
    "    - a bathroom with a glass darry proceinalllist appliances\n",
    "    - a nice kitchen with plated oblend laying inside a bagoox out on the floor\n",
    "    - a large commer table it and a pink shirt is like at a cemech\n",
    "    - a  gardage cookier down a street next to a train\n",
    "    - two colessued and paper tys most fisce entign cutting a large scall bowl\n",
    "6. The model Keras uses predicts characters one at a time based on the past 40 words in the text (including the part of the word being predicted one character at a time).  This means that the input size is of size 40 x numChars.\n",
    "7. This type of model could be used to generate names (for people, pets, places...) one character at a time.  One could also have the output of some audio recognition network be used in the places of the 'S' character in order to generate text captions for spoken word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size:0.8em;color:#888;text-align:center;padding-top:20px;\">If you find any errors or omissions in this material please contact me at vicente@cs.virginia.edu"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
